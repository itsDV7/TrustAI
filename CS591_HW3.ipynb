{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Q1.a\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 50)\n",
        "        self.fc2 = nn.Linear(50, 50)\n",
        "        self.fc3 = nn.Linear(50, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 784)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "def pgd_untargeted(model, x, y, k, eps, eps_step):\n",
        "    model.eval()\n",
        "    x_adv = x.clone().detach() + torch.zeros_like(x).uniform_(-eps, eps).to(x.device)\n",
        "    x_adv = torch.clamp(x_adv, 0, 1)\n",
        "\n",
        "    for i in range(k):\n",
        "        x_adv.requires_grad_(True)\n",
        "        with torch.enable_grad():\n",
        "            outputs = model(x_adv)\n",
        "            loss = nn.CrossEntropyLoss()(outputs, y)\n",
        "            grad = torch.autograd.grad(loss, x_adv)[0]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            x_adv = x_adv + eps_step * grad.sign()\n",
        "            delta = torch.clamp(x_adv - x, -eps, eps)\n",
        "            x_adv = torch.clamp(x + delta, 0, 1)\n",
        "            x_adv = x_adv.detach()\n",
        "\n",
        "    return x_adv\n",
        "\n",
        "def train_with_ibp(model, train_loader, test_loader, epochs, target_eps=0.1, device=\"cuda\"):\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss().to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    warmup_steps = 2000\n",
        "    rampup_steps = 10000\n",
        "    total_steps = epochs * len(train_loader)\n",
        "\n",
        "    lr_decay_steps = [15000, 25000]\n",
        "    step = 0\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            batch_size = data.size(0)\n",
        "\n",
        "            if step in lr_decay_steps:\n",
        "                for param_group in optimizer.param_groups:\n",
        "                    param_group['lr'] *= 0.1\n",
        "\n",
        "            if step < warmup_steps:\n",
        "                eps = 0\n",
        "                kappa = 1.0\n",
        "            else:\n",
        "                eps = min(target_eps * (step - warmup_steps) / rampup_steps, target_eps)\n",
        "                kappa = max(0.5, 1.0 - 0.5 * (step - warmup_steps) / rampup_steps)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            natural_output = model(data)\n",
        "            natural_loss = criterion(natural_output, target)\n",
        "\n",
        "            if eps > 0:\n",
        "                lower = data.view(-1, 784) - eps\n",
        "                upper = data.view(-1, 784) + eps\n",
        "\n",
        "                w1 = model.fc1.weight\n",
        "                b1 = model.fc1.bias\n",
        "                mu1 = (upper + lower) / 2\n",
        "                r1 = (upper - lower) / 2\n",
        "                center1 = torch.matmul(mu1, w1.t()) + b1\n",
        "                radius1 = torch.matmul(r1, torch.abs(w1.t()))\n",
        "                lower = torch.clamp(center1 - radius1, min=0)\n",
        "                upper = torch.clamp(center1 + radius1, min=0)\n",
        "\n",
        "                w2 = model.fc2.weight\n",
        "                b2 = model.fc2.bias\n",
        "                mu2 = (upper + lower) / 2\n",
        "                r2 = (upper - lower) / 2\n",
        "                center2 = torch.matmul(mu2, w2.t()) + b2\n",
        "                radius2 = torch.matmul(r2, torch.abs(w2.t()))\n",
        "                lower = torch.clamp(center2 - radius2, min=0)\n",
        "                upper = torch.clamp(center2 + radius2, min=0)\n",
        "\n",
        "                w3 = model.fc3.weight\n",
        "                b3 = model.fc3.bias\n",
        "                mu3 = (upper + lower) / 2\n",
        "                r3 = (upper - lower) / 2\n",
        "                center3 = torch.matmul(mu3, w3.t()) + b3\n",
        "                radius3 = torch.matmul(r3, torch.abs(w3.t()))\n",
        "                lower = center3 - radius3\n",
        "                upper = center3 + radius3\n",
        "\n",
        "                worst_case_logits = torch.zeros_like(natural_output).to(device)\n",
        "\n",
        "                for i in range(batch_size):\n",
        "                    for class_idx in range(10):\n",
        "                        if class_idx == target[i]:\n",
        "                            worst_case_logits[i, class_idx] = lower[i, class_idx]\n",
        "                        else:\n",
        "                            worst_case_logits[i, class_idx] = upper[i, class_idx]\n",
        "\n",
        "                robust_loss = criterion(worst_case_logits, target)\n",
        "            else:\n",
        "                robust_loss = natural_loss\n",
        "\n",
        "            loss = kappa * natural_loss + (1 - kappa) * robust_loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if batch_idx % 100 == 0:\n",
        "                print(f'Epoch {epoch+1}/{epochs}, Batch {batch_idx}/{len(train_loader)}, '\n",
        "                      f'Loss: {loss.item():.4f}, eps: {eps:.4f}, kappa: {kappa:.4f}, '\n",
        "                      f'LR: {optimizer.param_groups[0][\"lr\"]}')\n",
        "\n",
        "            step += 1\n",
        "\n",
        "    training_time = time.time() - start_time\n",
        "    accuracy, robust_accuracy = evaluate_model(model, test_loader, target_eps, device)\n",
        "\n",
        "    return accuracy, robust_accuracy, training_time\n",
        "\n",
        "def evaluate_model(model, test_loader, target_eps, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    robust_correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for data, target in test_loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        total += target.size(0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(data)\n",
        "            pred = output.argmax(dim=1)\n",
        "            correct += pred.eq(target).sum().item()\n",
        "\n",
        "        x_adv = pgd_untargeted(model, data, target, k=200, eps=target_eps, eps_step=target_eps/4)\n",
        "        with torch.no_grad():\n",
        "            output = model(x_adv)\n",
        "            robust_pred = output.argmax(dim=1)\n",
        "            robust_correct += robust_pred.eq(target).sum().item()\n",
        "\n",
        "    accuracy = 100. * correct / total\n",
        "    robust_accuracy = 100. * robust_correct / total\n",
        "\n",
        "    return accuracy, robust_accuracy\n",
        "\n",
        "def main():\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
        "    test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, pin_memory=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False, pin_memory=True)\n",
        "\n",
        "    print(\"Starting IBP training...\")\n",
        "    model_ibp = SimpleNN().to(device)\n",
        "    accuracy_ibp, robust_accuracy_ibp, ibp_time = train_with_ibp(\n",
        "        model_ibp, train_loader, test_loader, epochs=10, target_eps=0.1, device=device\n",
        "    )\n",
        "\n",
        "    print(f\"\\nIBP Training Results:\")\n",
        "    print(f\"Standard Accuracy: {accuracy_ibp:.2f}%\")\n",
        "    print(f\"Robust Accuracy (PGD): {robust_accuracy_ibp:.2f}%\")\n",
        "    print(f\"Training Time: {ibp_time:.2f} seconds\")\n",
        "\n",
        "    print(\"\\nStarting standard training...\")\n",
        "    model_std = SimpleNN().to(device)\n",
        "    start_time = time.time()\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss().to(device)\n",
        "    optimizer = optim.Adam(model_std.parameters(), lr=0.001)\n",
        "\n",
        "    for epoch in range(5):\n",
        "        model_std.train()\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model_std(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    standard_time = time.time() - start_time\n",
        "\n",
        "    standard_accuracy, standard_robust_accuracy = evaluate_model(\n",
        "        model_std, test_loader, 0.1, device\n",
        "    )\n",
        "\n",
        "    print(f\"\\nStandard Training Results:\")\n",
        "    print(f\"Standard Accuracy: {standard_accuracy:.2f}%\")\n",
        "    print(f\"Robust Accuracy (PGD): {standard_robust_accuracy:.2f}%\")\n",
        "    print(f\"Training Time: {standard_time:.2f} seconds\")\n",
        "\n",
        "    torch.save(model_ibp.state_dict(), 'ibp_model.pth')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "bXc05FvirAws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q1.b\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def interval_analysis(model, x, epsilon, device):\n",
        "    model.eval()\n",
        "    x = x.view(-1, 784)\n",
        "\n",
        "    lower_bound = x - epsilon\n",
        "    upper_bound = x + epsilon\n",
        "\n",
        "    weights = model.fc1.weight\n",
        "    biases = model.fc1.bias\n",
        "    lower_output = torch.matmul(lower_bound, weights.t()) + biases\n",
        "    upper_output = torch.matmul(upper_bound, weights.t()) + biases\n",
        "    lower_bound = torch.clamp(lower_output, min=0)\n",
        "    upper_bound = torch.clamp(upper_output, min=0)\n",
        "\n",
        "    weights = model.fc2.weight\n",
        "    biases = model.fc2.bias\n",
        "    lower_output = torch.matmul(lower_bound, weights.t()) + biases\n",
        "    upper_output = torch.matmul(upper_bound, weights.t()) + biases\n",
        "    lower_bound = torch.clamp(lower_output, min=0)\n",
        "    upper_bound = torch.clamp(upper_output, min=0)\n",
        "\n",
        "    weights = model.fc3.weight\n",
        "    biases = model.fc3.bias\n",
        "    lower_output = torch.matmul(lower_bound, weights.t()) + biases\n",
        "    upper_output = torch.matmul(upper_bound, weights.t()) + biases\n",
        "\n",
        "    return lower_output, upper_output\n",
        "\n",
        "def verify_robustness(model, test_loader, epsilon_values, device):\n",
        "    model.eval()\n",
        "    verified_correct = [0] * len(epsilon_values)\n",
        "    total = 0\n",
        "    non_robust_examples = {eps: [] for eps in epsilon_values}\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(test_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        batch_size = data.size(0)\n",
        "        total += batch_size\n",
        "\n",
        "        for eps_idx, epsilon in enumerate(epsilon_values):\n",
        "            lower_bounds, upper_bounds = interval_analysis(model, data, epsilon, device)\n",
        "\n",
        "            for i in range(batch_size):\n",
        "                true_class = target[i]\n",
        "                is_robust = True\n",
        "\n",
        "                for other_class in range(10):\n",
        "                    if other_class != true_class:\n",
        "                        if lower_bounds[i, true_class] <= upper_bounds[i, other_class]:\n",
        "                            is_robust = False\n",
        "                            if len(non_robust_examples[epsilon]) < 2:\n",
        "                                non_robust_examples[epsilon].append({\n",
        "                                    'image': data[i].cpu(),\n",
        "                                    'true_label': true_class.item(),\n",
        "                                    'bounds': (lower_bounds[i].cpu(), upper_bounds[i].cpu())\n",
        "                                })\n",
        "                            break\n",
        "\n",
        "                if is_robust:\n",
        "                    verified_correct[eps_idx] += 1\n",
        "\n",
        "        if batch_idx % 10 == 0:\n",
        "            print(f\"Processed {batch_idx * batch_size}/{total} images\")\n",
        "\n",
        "    verified_accuracies = [100 * correct / total for correct in verified_correct]\n",
        "    return verified_accuracies, non_robust_examples\n",
        "\n",
        "def plot_verification_results(epsilon_values, verified_accuracies):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(epsilon_values, verified_accuracies, marker='o')\n",
        "    plt.xlabel('ε')\n",
        "    plt.ylabel('Verified Accuracy (%)')\n",
        "    plt.title('Verified Accuracy vs Perturbation Size')\n",
        "    plt.grid(True)\n",
        "    plt.savefig('verification_results.png')\n",
        "    plt.close()\n",
        "\n",
        "def analyze_non_robust_examples(non_robust_examples, epsilon_values, model, device):\n",
        "    fig, axes = plt.subplots(len(epsilon_values), 2, figsize=(10, 4*len(epsilon_values)))\n",
        "\n",
        "    for i, eps in enumerate(epsilon_values):\n",
        "        if not non_robust_examples[eps]:\n",
        "            continue\n",
        "\n",
        "        example = non_robust_examples[eps][0]\n",
        "        image = example['image']\n",
        "        true_label = example['true_label']\n",
        "        lower_bounds, upper_bounds = example['bounds']\n",
        "\n",
        "        axes[i, 0].imshow(image.squeeze(), cmap='gray')\n",
        "        axes[i, 0].set_title(f'ε={eps:.3f}, True Label: {true_label}')\n",
        "\n",
        "        x_adv = pgd_untargeted(model,\n",
        "                              image.unsqueeze(0).to(device),\n",
        "                              torch.tensor([true_label]).to(device),\n",
        "                              k=200,\n",
        "                              eps=eps,\n",
        "                              eps_step=eps/4)\n",
        "\n",
        "        axes[i, 1].imshow(x_adv.squeeze().cpu(), cmap='gray')\n",
        "        with torch.no_grad():\n",
        "            pred = model(x_adv).argmax(dim=1).item()\n",
        "        axes[i, 1].set_title(f'Adversarial Example\\nPredicted: {pred}')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('non_robust_examples.png')\n",
        "    plt.close()\n",
        "\n",
        "def main():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    model = SimpleNN().to(device)\n",
        "    model.load_state_dict(torch.load('ibp_model.pth'))\n",
        "\n",
        "    transform = transforms.Compose([transforms.ToTensor()])\n",
        "    test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=100, shuffle=False, pin_memory=True)\n",
        "\n",
        "    epsilon_values = np.linspace(0.01, 0.1, 10)\n",
        "\n",
        "    print(\"Starting verification...\")\n",
        "    verified_accuracies, non_robust_examples = verify_robustness(\n",
        "        model, test_loader, epsilon_values, device\n",
        "    )\n",
        "\n",
        "    print(\"\\nVerification Results:\")\n",
        "    for eps, acc in zip(epsilon_values, verified_accuracies):\n",
        "        print(f\"ε = {eps:.3f}: {acc:.2f}% verified accuracy\")\n",
        "\n",
        "    plot_verification_results(epsilon_values, verified_accuracies)\n",
        "\n",
        "    analyze_non_robust_examples(non_robust_examples, epsilon_values, model, device)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "C9qHzEjgtivM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q2\n",
        "\n",
        "\n",
        "class BOWModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=300, hidden_dim=100, num_classes=2):\n",
        "        super(BOWModel, self).__init__()\n",
        "\n",
        "        self.g_word = nn.Linear(embed_dim, embed_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        self.fc1 = nn.Linear(embed_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.g_word(x))\n",
        "        x = torch.mean(x, dim=1)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "def train_with_ibp(model, train_loader, test_loader, epochs, target_eps=0.1, device=\"cuda\"):\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss().to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "\n",
        "    warmup_steps = 2000\n",
        "    rampup_steps = 10000\n",
        "    total_steps = epochs * len(train_loader)\n",
        "    step = 0\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            batch_size = data.size(0)\n",
        "\n",
        "            if step < warmup_steps:\n",
        "                eps = 0\n",
        "                kappa = 1.0\n",
        "            else:\n",
        "                eps = min(target_eps * (step - warmup_steps) / rampup_steps, target_eps)\n",
        "                kappa = max(0.5, 1.0 - 0.5 * (step - warmup_steps) / rampup_steps)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            natural_output = model(data)\n",
        "            natural_loss = criterion(natural_output, target)\n",
        "\n",
        "            if eps > 0:\n",
        "                lower = data - eps\n",
        "                upper = data + eps\n",
        "\n",
        "                w1 = model.g_word.weight\n",
        "                b1 = model.g_word.bias\n",
        "                mu1 = (upper + lower) / 2\n",
        "                r1 = (upper - lower) / 2\n",
        "                center1 = torch.matmul(mu1.view(-1, mu1.size(-1)), w1.t()) + b1\n",
        "                radius1 = torch.matmul(r1.view(-1, r1.size(-1)), torch.abs(w1.t()))\n",
        "                lower = torch.clamp(center1 - radius1, min=0)\n",
        "                upper = torch.clamp(center1 + radius1, min=0)\n",
        "                lower = lower.view(batch_size, -1, lower.size(-1))\n",
        "                upper = upper.view(batch_size, -1, upper.size(-1))\n",
        "                lower = torch.mean(lower, dim=1)\n",
        "                upper = torch.mean(upper, dim=1)\n",
        "\n",
        "                w2 = model.fc1.weight\n",
        "                b2 = model.fc1.bias\n",
        "                mu2 = (upper + lower) / 2\n",
        "                r2 = (upper - lower) / 2\n",
        "                center2 = torch.matmul(mu2, w2.t()) + b2\n",
        "                radius2 = torch.matmul(r2, torch.abs(w2.t()))\n",
        "                lower = torch.clamp(center2 - radius2, min=0)\n",
        "                upper = torch.clamp(center2 + radius2, min=0)\n",
        "\n",
        "                w3 = model.fc2.weight\n",
        "                b3 = model.fc2.bias\n",
        "                mu3 = (upper + lower) / 2\n",
        "                r3 = (upper - lower) / 2\n",
        "                center3 = torch.matmul(mu3, w3.t()) + b3\n",
        "                radius3 = torch.matmul(r3, torch.abs(w3.t()))\n",
        "                lower = torch.clamp(center3 - radius3, min=0)\n",
        "                upper = torch.clamp(center3 + radius3, min=0)\n",
        "\n",
        "                w4 = model.fc3.weight\n",
        "                b4 = model.fc3.bias\n",
        "                mu4 = (upper + lower) / 2\n",
        "                r4 = (upper - lower) / 2\n",
        "                center4 = torch.matmul(mu4, w4.t()) + b4\n",
        "                radius4 = torch.matmul(r4, torch.abs(w4.t()))\n",
        "                lower = center4 - radius4\n",
        "                upper = center4 + radius4\n",
        "\n",
        "                worst_case_logits = torch.zeros_like(natural_output).to(device)\n",
        "\n",
        "                for i in range(batch_size):\n",
        "                    for class_idx in range(2):\n",
        "                        if class_idx == target[i]:\n",
        "                            worst_case_logits[i, class_idx] = lower[i, class_idx]\n",
        "                        else:\n",
        "                            worst_case_logits[i, class_idx] = upper[i, class_idx]\n",
        "\n",
        "                robust_loss = criterion(worst_case_logits, target)\n",
        "            else:\n",
        "                robust_loss = natural_loss\n",
        "\n",
        "            loss = kappa * natural_loss + (1 - kappa) * robust_loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if batch_idx % 100 == 0:\n",
        "                print(f'Epoch {epoch+1}/{epochs}, Batch {batch_idx}/{len(train_loader)}, '\n",
        "                      f'Loss: {loss.item():.4f}, eps: {eps:.4f}, kappa: {kappa:.4f}')\n",
        "\n",
        "            step += 1\n",
        "\n",
        "        model.eval()\n",
        "        test_loss = 0\n",
        "        correct = 0\n",
        "        robust_correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for data, target in test_loader:\n",
        "                data, target = data.to(device), target.to(device)\n",
        "                batch_size = data.size(0)\n",
        "                total += batch_size\n",
        "\n",
        "                output = model(data)\n",
        "                test_loss += criterion(output, target).item() * batch_size\n",
        "                pred = output.argmax(dim=1)\n",
        "                correct += pred.eq(target).sum().item()\n",
        "\n",
        "                if eps > 0:\n",
        "                    lower = data - eps\n",
        "                    upper = data + eps\n",
        "\n",
        "                    w1 = model.g_word.weight\n",
        "                    b1 = model.g_word.bias\n",
        "                    mu1 = (upper + lower) / 2\n",
        "                    r1 = (upper - lower) / 2\n",
        "                    center1 = torch.matmul(mu1.view(-1, mu1.size(-1)), w1.t()) + b1\n",
        "                    radius1 = torch.matmul(r1.view(-1, r1.size(-1)), torch.abs(w1.t()))\n",
        "                    lower = torch.clamp(center1 - radius1, min=0)\n",
        "                    upper = torch.clamp(center1 + radius1, min=0)\n",
        "                    lower = lower.view(batch_size, -1, lower.size(-1))\n",
        "                    upper = upper.view(batch_size, -1, upper.size(-1))\n",
        "                    lower = torch.mean(lower, dim=1)\n",
        "                    upper = torch.mean(upper, dim=1)\n",
        "\n",
        "                    w2 = model.fc1.weight\n",
        "                    b2 = model.fc1.bias\n",
        "                    mu2 = (upper + lower) / 2\n",
        "                    r2 = (upper - lower) / 2\n",
        "                    center2 = torch.matmul(mu2, w2.t()) + b2\n",
        "                    radius2 = torch.matmul(r2, torch.abs(w2.t()))\n",
        "                    lower = torch.clamp(center2 - radius2, min=0)\n",
        "                    upper = torch.clamp(center2 + radius2, min=0)\n",
        "\n",
        "                    w3 = model.fc2.weight\n",
        "                    b3 = model.fc2.bias\n",
        "                    mu3 = (upper + lower) / 2\n",
        "                    r3 = (upper - lower) / 2\n",
        "                    center3 = torch.matmul(mu3, w3.t()) + b3\n",
        "                    radius3 = torch.matmul(r3, torch.abs(w3.t()))\n",
        "                    lower = torch.clamp(center3 - radius3, min=0)\n",
        "                    upper = torch.clamp(center3 + radius3, min=0)\n",
        "\n",
        "                    w4 = model.fc3.weight\n",
        "                    b4 = model.fc3.bias\n",
        "                    mu4 = (upper + lower) / 2\n",
        "                    r4 = (upper - lower) / 2\n",
        "                    center4 = torch.matmul(mu4, w4.t()) + b4\n",
        "                    radius4 = torch.matmul(r4, torch.abs(w4.t()))\n",
        "                    lower = center4 - radius4\n",
        "                    upper = center4 + radius4\n",
        "\n",
        "                    worst_case_logits = torch.zeros_like(output).to(device)\n",
        "                    for i in range(batch_size):\n",
        "                        for class_idx in range(2):\n",
        "                            if class_idx == target[i]:\n",
        "                                worst_case_logits[i, class_idx] = lower[i, class_idx]\n",
        "                            else:\n",
        "                                worst_case_logits[i, class_idx] = upper[i, class_idx]\n",
        "\n",
        "                    robust_correct += (worst_case_logits.argmax(dim=1) == target).sum().item()\n",
        "\n",
        "        test_loss /= total\n",
        "        accuracy = 100. * correct / total\n",
        "        robust_accuracy = 100. * robust_correct / total if eps > 0 else accuracy\n",
        "\n",
        "        print(f'\\nTest set: Average loss: {test_loss:.4f}, '\n",
        "              f'Accuracy: {accuracy:.2f}%, Robust Accuracy: {robust_accuracy:.2f}%')\n",
        "\n",
        "    training_time = time.time() - start_time\n",
        "    return accuracy, robust_accuracy, training_time\n",
        "\n",
        "def evaluate_model(model, test_loader, target_eps, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    robust_correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            total += target.size(0)\n",
        "\n",
        "            output = model(data)\n",
        "            pred = output.argmax(dim=1)\n",
        "            correct += pred.eq(target).sum().item()\n",
        "\n",
        "            if target_eps > 0:\n",
        "                lower = data - target_eps\n",
        "                upper = data + target_eps\n",
        "                robust_correct += (worst_case_logits.argmax(dim=1) == target).sum().item()\n",
        "\n",
        "    accuracy = 100. * correct / total\n",
        "    robust_accuracy = 100. * robust_correct / total\n",
        "\n",
        "    return accuracy, robust_accuracy"
      ],
      "metadata": {
        "id": "zfhSoTEyzy9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q2\n",
        "\n",
        "# Generated using GPT to import GloVe cause torchtext was not working on my device.\n",
        "def download_glove_embeddings():\n",
        "    \"\"\"Download GloVe embeddings if not already present.\"\"\"\n",
        "    url = \"https://nlp.stanford.edu/data/glove.6B.zip\"\n",
        "    if not os.path.exists('glove.6B'):\n",
        "        os.makedirs('glove.6B')\n",
        "\n",
        "    zip_path = 'glove.6B/glove.6B.zip'\n",
        "    if not os.path.exists(zip_path):\n",
        "        print(\"Downloading GloVe embeddings...\")\n",
        "        response = requests.get(url)\n",
        "        with open(zip_path, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "\n",
        "        import zipfile\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall('glove.6B')\n",
        "\n",
        "# Generated using GPT to import GloVe cause torchtext was not working on my device.\n",
        "def load_glove_embeddings(vocab_size=50000, embedding_dim=300):\n",
        "    \"\"\"Load GloVe embeddings and create embedding matrix.\"\"\"\n",
        "    print(\"Loading GloVe embeddings...\")\n",
        "\n",
        "    # Download embeddings if needed\n",
        "    download_glove_embeddings()\n",
        "\n",
        "    embeddings_dict = {}\n",
        "    word_to_idx = {'<pad>': 0}  # Add padding token\n",
        "    vectors = [[0.] * embedding_dim]  # Add padding vector\n",
        "\n",
        "    # Load embeddings from file\n",
        "    with open(f'glove.6B/glove.6B.{embedding_dim}d.txt', 'r', encoding='utf-8') as f:\n",
        "        for i, line in enumerate(tqdm(f, desc=\"Loading embeddings\")):\n",
        "            if i >= vocab_size - 1:  # -1 because we added padding\n",
        "                break\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vector = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_dict[word] = vector\n",
        "            word_to_idx[word] = len(vectors)\n",
        "            vectors.append(vector)\n",
        "\n",
        "    print(f\"Loaded {len(vectors)} word vectors\")\n",
        "    return np.array(vectors), word_to_idx\n",
        "\n",
        "\n",
        "\n",
        "def preprocess_data(texts, word_to_idx, max_len=500):\n",
        "    sequences = []\n",
        "    for text in tqdm(texts, desc=\"Processing texts\"):\n",
        "        sequence = []\n",
        "        for word in text.lower().split():\n",
        "            if word in word_to_idx:\n",
        "                sequence.append(word_to_idx[word])\n",
        "        if len(sequence) > max_len:\n",
        "            sequence = sequence[:max_len]\n",
        "        else:\n",
        "            sequence = sequence + [0] * (max_len - len(sequence))\n",
        "        sequences.append(sequence)\n",
        "    return np.array(sequences)\n",
        "\n",
        "def create_datasets(train_texts, train_labels, test_texts, test_labels, embeddings, batch_size=32):\n",
        "    train_embedded = torch.FloatTensor(np.array([\n",
        "        [embeddings[idx] for idx in sequence]\n",
        "        for sequence in tqdm(train_texts)\n",
        "    ]))\n",
        "\n",
        "    test_embedded = torch.FloatTensor(np.array([\n",
        "        [embeddings[idx] for idx in sequence]\n",
        "        for sequence in tqdm(test_texts)\n",
        "    ]))\n",
        "\n",
        "    train_dataset = TensorDataset(\n",
        "        train_embedded,\n",
        "        torch.LongTensor(train_labels)\n",
        "    )\n",
        "    test_dataset = TensorDataset(\n",
        "        test_embedded,\n",
        "        torch.LongTensor(test_labels)\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        pin_memory=True\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    return train_loader, test_loader\n",
        "\n",
        "def load_imdb_dataset():\n",
        "    dataset = load_dataset(\"imdb\")\n",
        "    train_texts = dataset['train']['text']\n",
        "    train_labels = dataset['train']['label']\n",
        "    test_texts = dataset['test']['text']\n",
        "    test_labels = dataset['test']['label']\n",
        "\n",
        "    return train_texts, train_labels, test_texts, test_labels"
      ],
      "metadata": {
        "id": "kBGhTVTn5gVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_standard(model, train_loader, test_loader, epochs, device=\"cuda\"):\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss().to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if batch_idx % 100 == 0:\n",
        "                print(f'Epoch {epoch+1}/{epochs}, Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}')\n",
        "\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for data, target in test_loader:\n",
        "                data, target = data.to(device), target.to(device)\n",
        "                output = model(data)\n",
        "                pred = output.argmax(dim=1)\n",
        "                total += target.size(0)\n",
        "                correct += pred.eq(target).sum().item()\n",
        "\n",
        "        accuracy = 100. * correct / total\n",
        "        print(f'\\nTest set: Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "    training_time = time.time() - start_time\n",
        "    return accuracy, training_time\n",
        "\n",
        "def main():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    torch.manual_seed(42)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(42)\n",
        "\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    dataset = load_dataset(\"imdb\")\n",
        "\n",
        "    train_texts = dataset['train']['text']\n",
        "    train_labels = dataset['train']['label']\n",
        "    test_texts = dataset['test']['text']\n",
        "    test_labels = dataset['test']['label']\n",
        "\n",
        "    embeddings, word_to_idx = load_glove_embeddings()\n",
        "    vocab_size = len(embeddings)\n",
        "\n",
        "    train_sequences = preprocess_data(train_texts, word_to_idx)\n",
        "    test_sequences = preprocess_data(test_texts, word_to_idx)\n",
        "\n",
        "    train_loader, test_loader = create_datasets(\n",
        "        train_sequences, train_labels,\n",
        "        test_sequences, test_labels,\n",
        "        embeddings,\n",
        "        batch_size=32\n",
        "    )\n",
        "\n",
        "    print(\"\\n=== Standard Training ===\")\n",
        "    model_std = BOWModel(vocab_size=vocab_size, embed_dim=300, hidden_dim=100, num_classes=2)\n",
        "    accuracy_std, time_std = train_standard(\n",
        "        model=model_std,\n",
        "        train_loader=train_loader,\n",
        "        test_loader=test_loader,\n",
        "        epochs=10,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    print(\"\\n=== IBP Training ===\")\n",
        "    model_ibp = BOWModel(vocab_size=vocab_size, embed_dim=300, hidden_dim=100, num_classes=2)\n",
        "    accuracy_ibp, robust_accuracy_ibp, time_ibp = train_with_ibp(\n",
        "        model=model_ibp,\n",
        "        train_loader=train_loader,\n",
        "        test_loader=test_loader,\n",
        "        epochs=10,\n",
        "        target_eps=0.1,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    print(\"\\n=== Final Results ===\")\n",
        "    print(f\"Standard Training:\")\n",
        "    print(f\"- Standard Accuracy: {accuracy_std:.2f}%\")\n",
        "    print(f\"- Training Time: {time_std:.2f} seconds\")\n",
        "    print(f\"\\nIBP Training:\")\n",
        "    print(f\"- Standard Accuracy: {accuracy_ibp:.2f}%\")\n",
        "    print(f\"- Verified Accuracy: {robust_accuracy_ibp:.2f}%\")\n",
        "    print(f\"- Training Time: {time_ibp:.2f} seconds\")\n",
        "    print(f\"\\nComparison:\")\n",
        "    print(f\"- Training Time Ratio (IBP/Standard): {time_ibp/time_std:.2f}x\")\n",
        "    print(f\"- Accuracy Drop from IBP: {accuracy_std - accuracy_ibp:.2f}%\")\n",
        "\n",
        "    torch.save(model_std.state_dict(), 'bow_standard_model.pth')\n",
        "    torch.save(model_ibp.state_dict(), 'bow_ibp_model.pth')\n",
        "    print(\"\\nModels saved as bow_standard_model.pth and bow_ibp_model.pth\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "jNhgyYbz6i42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l5vYwSYE-n4q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}